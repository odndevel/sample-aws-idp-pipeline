import json
import os
import traceback

import yaml
from strands import Agent
from strands.models import BedrockModel
from strands.types.content import SystemContentBlock

from shared.ddb_client import (
    update_workflow_status,
    record_step_start,
    record_step_complete,
    record_step_error,
    get_project_language,
    StepName,
    WorkflowStatus,
)
from shared.s3_analysis import get_all_segment_analyses, save_summary

BATCH_SIZE = 150
BATCH_OVERLAP = 30
PROMPTS = None


def get_prompts():
    global PROMPTS
    if PROMPTS is None:
        path = os.path.join(os.path.dirname(__file__), 'prompts', 'summarizer.yaml')
        with open(path, 'r') as f:
            PROMPTS = yaml.safe_load(f)
    return PROMPTS


def collect_page_descriptions(segments):
    """Collect page descriptions from segment data (generated by AnalysisFinalizer)."""
    page_descriptions = []
    for seg in segments:
        page_num = seg.get('segment_index', 0) + 1
        description = seg.get('page_description', '')
        page_descriptions.append({
            'page': page_num,
            'description': description
        })
    return page_descriptions


def format_descriptions_for_input(page_descriptions):
    """Format page descriptions as text for LLM input."""
    parts = []
    for pd in page_descriptions:
        page = pd.get('page', 0)
        desc = pd.get('description', '')
        parts.append(f'Page {page}: {desc}')
    return '\n\n'.join(parts)


def build_system_prompt(text, use_cache=False):
    """Build system prompt, optionally with cache point for multi-batch scenarios."""
    if not use_cache:
        return text
    return [
        SystemContentBlock(text=text),
        SystemContentBlock(cachePoint={'type': 'default'}),
    ]


def batch_with_overlap(items, batch_size, overlap):
    """Split items into batches with overlap between consecutive batches."""
    if len(items) <= batch_size:
        return [items]
    batches = []
    step = batch_size - overlap
    for start in range(0, len(items), step):
        end = min(start + batch_size, len(items))
        batches.append(items[start:end])
        if end >= len(items):
            break
    return batches


def generate_document_summary(model_id, region, language, page_descriptions, total_pages):
    """Generate document summary using plain text response with batching."""
    prompts = get_prompts()
    system_text = prompts['document_summary_system']

    batches = batch_with_overlap(page_descriptions, BATCH_SIZE, BATCH_OVERLAP)
    use_cache = len(batches) > 1

    bedrock_model = BedrockModel(model_id=model_id, region_name=region)
    system_prompt = build_system_prompt(system_text, use_cache=use_cache)

    if len(batches) == 1:
        descriptions_text = format_descriptions_for_input(page_descriptions)
        user_text = prompts['document_summary_user'].format(
            total_pages=total_pages,
            language=language,
            page_descriptions=descriptions_text
        )

        print(f'Document summary: Single call')

        try:
            agent = Agent(model=bedrock_model, system_prompt=system_prompt)
            result = agent(user_text)
            return str(result).strip()
        except Exception as e:
            print(f'Document summary failed: {e}')
            return ''

    partial_summaries = []

    for batch_idx, batch in enumerate(batches):
        batch_text = format_descriptions_for_input(batch)
        batch_page_nums = [pd.get('page', 0) for pd in batch]
        user_text = prompts['document_summary_user'].format(
            total_pages=f'{batch_page_nums[0]}-{batch_page_nums[-1]} of {total_pages}',
            language=language,
            page_descriptions=batch_text
        )

        print(f'Document summary: Batch {batch_idx + 1}/{len(batches)} '
              f'(pages {batch_page_nums[0]}-{batch_page_nums[-1]})')

        try:
            agent = Agent(model=bedrock_model, system_prompt=system_prompt)
            partial = str(agent(user_text)).strip()
            if partial:
                partial_summaries.append(
                    f'[Pages {batch_page_nums[0]}-{batch_page_nums[-1]}]\n{partial}'
                )
        except Exception as e:
            print(f'Document summary batch {batch_idx + 1} failed: {e}')

    if not partial_summaries:
        return ''

    if len(partial_summaries) == 1:
        return partial_summaries[0]

    merge_text = '\n\n'.join(partial_summaries)
    merge_user = (
        f'Below are section summaries of a {total_pages}-page document. '
        f'Write a unified comprehensive summary.\n'
        f'Respond ONLY in: {language}\n\n{merge_text}'
    )

    print(f'Document summary: Merging {len(partial_summaries)} partial summaries')

    try:
        agent = Agent(model=bedrock_model, system_prompt=system_prompt)
        return str(agent(merge_user)).strip()
    except Exception as e:
        print(f'Document summary merge failed: {e}')
        return '\n\n'.join(partial_summaries)


def extract_document_id_from_uri(file_uri: str) -> str:
    """Extract document_id from file_uri as fallback."""
    if not file_uri:
        return ''
    parts = file_uri.split('/')
    try:
        doc_index = parts.index('documents')
        if doc_index + 1 < len(parts):
            return parts[doc_index + 1]
    except ValueError:
        pass
    return ''


def handler(event, context):
    print(f'Event: {json.dumps(event)}')

    workflow_id = event.get('workflow_id')
    document_id = event.get('document_id')
    project_id = event.get('project_id')
    file_uri = event.get('file_uri')
    segment_count = event.get('segment_count', 0)

    summarizer_model_id = os.environ['SUMMARIZER_MODEL_ID']
    region = os.environ.get('AWS_REGION', 'us-east-1')

    if not document_id and file_uri:
        document_id = extract_document_id_from_uri(file_uri)
        print(f'Extracted document_id from file_uri: {document_id}')

    record_step_complete(workflow_id, StepName.SEGMENT_ANALYZER, segment_count=segment_count)
    record_step_start(workflow_id, StepName.DOCUMENT_SUMMARIZER)

    try:
        language = 'en'
        if project_id:
            language = get_project_language(project_id)
        print(f'Using language: {language}')

        segments = get_all_segment_analyses(file_uri, segment_count)

        if not segments:
            print(f'No segments found in S3 for file {file_uri}')
            record_step_complete(
                workflow_id,
                StepName.DOCUMENT_SUMMARIZER,
                skipped=True,
                reason='No segments found'
            )
            return {
                'workflow_id': workflow_id,
                'status': 'no_segments',
                'message': 'No segments found for summarization'
            }

        segments_sorted = sorted(segments, key=lambda x: x.get('segment_index', 0))
        total_pages = len(segments_sorted)

        # Collect page descriptions from segment data (generated by AnalysisFinalizer)
        page_descriptions = collect_page_descriptions(segments_sorted)
        descriptions_with_content = sum(1 for pd in page_descriptions if pd.get('description'))
        print(f'Collected {descriptions_with_content}/{total_pages} page descriptions from segments')

        # Generate document summary
        document_summary = generate_document_summary(
            summarizer_model_id, region, language, page_descriptions, total_pages
        )
        print(f'Document summary complete: {len(document_summary)} chars')

        pages = []
        for pd in page_descriptions:
            pages.append({
                'page': pd.get('page', 0),
                'description': pd.get('description', '')
            })

        summary_data = {
            'language': language,
            'document_summary': document_summary,
            'total_pages': total_pages,
            'pages': pages
        }

        save_summary(file_uri, summary_data)
        print(f'Saved summary.json to S3 for file {file_uri}')

        record_step_complete(
            workflow_id,
            StepName.DOCUMENT_SUMMARIZER,
            segment_count=total_pages
        )

        update_workflow_status(
            document_id,
            workflow_id,
            WorkflowStatus.COMPLETED,
        )

        print(f'Completed workflow {workflow_id} with {total_pages} pages')

        return {
            'workflow_id': workflow_id,
            'status': 'completed',
            'segment_count': total_pages,
            'summary_pages': len(pages),
        }

    except Exception as e:
        print(f'Error in document summarization: {e}')
        traceback.print_exc()
        record_step_error(workflow_id, StepName.DOCUMENT_SUMMARIZER, str(e))
        update_workflow_status(document_id, workflow_id, WorkflowStatus.FAILED, error=str(e))
        return {
            'workflow_id': workflow_id,
            'status': 'failed',
            'error': str(e)
        }
